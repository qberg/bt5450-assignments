{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "neural-austin",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Supervised-Learning\" data-toc-modified-id=\"Supervised-Learning-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Supervised Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Derivation-of-Naive-Bayes-Posterior-Probability\" data-toc-modified-id=\"Derivation-of-Naive-Bayes-Posterior-Probability-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Derivation of Naive Bayes Posterior Probability</a></span><ul class=\"toc-item\"><li><span><a href=\"#Role-of-Priors-in-Naive-Bayes-Classifier\" data-toc-modified-id=\"Role-of-Priors-in-Naive-Bayes-Classifier-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Role of Priors in Naive Bayes Classifier</a></span></li></ul></li><li><span><a href=\"#Implementation-of-Naive-Bayes-Classifier\" data-toc-modified-id=\"Implementation-of-Naive-Bayes-Classifier-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Implementation of Naive Bayes Classifier</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fitting-the-model-using-two-features\" data-toc-modified-id=\"Fitting-the-model-using-two-features-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Fitting the model using two features</a></span></li><li><span><a href=\"#Fitting-the-model-using-all-features\" data-toc-modified-id=\"Fitting-the-model-using-all-features-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Fitting the model using all features</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-design",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-protection",
   "metadata": {},
   "source": [
    "## Derivation of Naive Bayes Posterior Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-westminster",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T14:58:04.148945Z",
     "start_time": "2021-09-01T14:58:04.133315Z"
    }
   },
   "source": [
    "* The various probablities involved are,\n",
    "\n",
    "    * **Posterior Probabiliry:** $P(Tt|x_1,x_2,\\dots,x_n)$\n",
    "    * **Likelihood:** $P(x_1,x_2,\\dots,x_n|Tt)$\n",
    "    * **Prior Probabiity:** $P(Tt)$\n",
    "---\n",
    "* Using the Bayes rule for conditional probabilities we can write,\n",
    "$$P(Tt|x_1,x_2,\\dots,x_n) = \\frac{P(x_1,x_2,\\dots,x_n|Tt)\\times P(Tt)}{P(x_1,x_2,\\dots,x_n)}$$\n",
    "* By independance,\n",
    "$$P(x_1,x_2,\\dots,x_n) = \\prod_{i=1}^{n} P(x_i)$$\n",
    "* The likelihood term can be manipulated using the chain rule and assumption of independant variables to give,\n",
    "$$P(x_1,x_2,\\dots,x_n|Tt) = \\prod_{i=1}^{n} P(x_i|Tt) $$\n",
    "* Combining all this we get,\n",
    "$$P(Tt|x_1,x_2,\\dots,x_n) = \\frac{\\prod_{i=1}^{n} P(x_i|Tt)\\times P(Tt)}{\\prod_{i=1}^{n} P(x_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-catholic",
   "metadata": {},
   "source": [
    "### Role of Priors in Naive Bayes Classifier\n",
    "\n",
    "* While building a classifier, we are interested in the posterior probablity calculated above. It can be defined as , given a sample test point what is the probability of it belonging to class Tt.\n",
    "* Therefore the classifications are made using argmax function for posterior probablity of the point belonging to various classes.\n",
    "* We can safely ignore the denominator for our decison rule, since it will be the same in all cases.\n",
    "* Thus,\n",
    "$$P(Tt|x_1,x_2,\\dots,x_n) \\propto \\prod_{i=1}^{n} P(x_i|Tt)\\times P(Tt)$$\n",
    "* As can be seen from the formula the prior plays an important role in classification.\n",
    "* When training a Naive Bayes Classifier, the prior probabilities are taken to be the frequency of occurences of samples belonging to the particular class in the training dataset.\n",
    "* Thus, if our training dataset has more samples belonging to a particular class, the corresponding prior probabilities will also be higher.\n",
    "* This will cause concerns if the number of training data points available for various classes is heavily skewed as there is a chance for the model to become biased towards the class with high prior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-dayton",
   "metadata": {},
   "source": [
    "## Implementation of Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "confused-exposure",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T06:13:38.057336Z",
     "start_time": "2021-09-03T06:13:37.476831Z"
    }
   },
   "outputs": [],
   "source": [
    "#Libraries to be used.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "uniform-primary",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T06:13:38.081336Z",
     "start_time": "2021-09-03T06:13:38.061343Z"
    }
   },
   "outputs": [],
   "source": [
    "#Utilities to be used.\n",
    "pp = pprint.PrettyPrinter(width=60, depth=1)\n",
    "\n",
    "\n",
    "#A class that helps with processing the csv files.\n",
    "class process_csv:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.features = []\n",
    "\n",
    "    def read_csv_to_df(self):\n",
    "        #A function that takes in csv files and returns a dataframe after some processing.\n",
    "        self.features = [*pd.read_csv(self.path, nrows=1)]\n",
    "        self.features.pop(\n",
    "            0)  #Removes the first coulmn since it is not a feature.\n",
    "        df = pd.read_csv(self.path, usecols=[col for col in self.features])\n",
    "        return df\n",
    "\n",
    "    def scale_features(self, df):\n",
    "        #A fucntion that scales the values in a dataframe.\n",
    "        df_scaled = df.apply(lambda seq: (seq.astype(float) - min(seq)) /\n",
    "                             (max(seq) - min(seq)),\n",
    "                             axis=0)\n",
    "        return df_scaled\n",
    "\n",
    "    def df_to_numpy(self, df, scaling):\n",
    "        #Converts a dataframe in to an numpy array.\n",
    "        df_scaled = self.scale_features(df)\n",
    "        if scaling:\n",
    "            return df_scaled.to_numpy()\n",
    "        else:\n",
    "            return df.to_numpy()\n",
    "\n",
    "    def select_class(self, class_name, no_of_features_is_two, scaling):\n",
    "        #A function that filters samples belonging to the malignant class from the training data.\n",
    "        df = self.read_csv_to_df()\n",
    "        label_for_class = (df['diagnosis'] == class_name)\n",
    "        if no_of_features_is_two:\n",
    "            class_df = df.loc[label_for_class,\n",
    "                              [\"concave points_worst\", \"radius_mean\"]]\n",
    "        else:\n",
    "            class_df = df.loc[label_for_class, self.features[3:]]\n",
    "        class_arr = self.df_to_numpy(class_df, scaling)\n",
    "        return class_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "diverse-princess",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T06:13:38.201887Z",
     "start_time": "2021-09-03T06:13:38.081336Z"
    }
   },
   "outputs": [],
   "source": [
    "#A class that performs classification using naive bayes.\n",
    "class Naive_Bayes_Classifier:\n",
    "    def __init__(self, X_train_M, X_train_B):\n",
    "        self.training_data = [X_train_M, X_train_B]\n",
    "        self.Classes = ['M', 'B']\n",
    "        self.params = dict((Class, {}) for Class in self.Classes)\n",
    "\n",
    "    def fit(self):\n",
    "        #A method that estimates the values of parameters from the training data.\n",
    "        #Unpacking the training data.\n",
    "        X_m, X_b = self.training_data\n",
    "        #List of various statisitical params as a list.\n",
    "        N = [X_m.shape[0], X_b.shape[0]]\n",
    "        arr_mean = [\n",
    "            np.mean(X_m, axis=0, dtype=np.float64),\n",
    "            np.mean(X_b, axis=0, dtype=np.float64)\n",
    "        ]\n",
    "        arr_std = [\n",
    "            np.std(X_m, axis=0, dtype=np.float64),\n",
    "            np.std(X_b, axis=0, dtype=np.float64)\n",
    "        ]\n",
    "        for Class in self.Classes:\n",
    "            class_index = self.Classes.index(Class)\n",
    "            self.params[Class]['Prior'] = N[class_index] / sum(N)\n",
    "            self.params[Class]['Mean'] = arr_mean[class_index]\n",
    "            self.params[Class]['Std'] = arr_std[class_index]\n",
    "        #return self.params\n",
    "\n",
    "    def log_likelihood(self, x, mu, sigma):\n",
    "        return np.sum(-np.log(sigma) - (x - mu)**2 / (2 * sigma**2))\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Calculates probabilities for each class per sample and assigns the class with maximum probability\n",
    "        pred = []\n",
    "        for i in range(X_test.shape[0]):\n",
    "            sample = X_test[i, :]\n",
    "            logp_m = self.log_likelihood(\n",
    "                sample, self.params['M']['Mean'],\n",
    "                self.params['M'][\"Std\"]) + self.params['M'][\"Prior\"]\n",
    "            logp_b = self.log_likelihood(\n",
    "                sample, self.params['B']['Mean'],\n",
    "                self.params['B'][\"Std\"]) + self.params['B'][\"Prior\"]\n",
    "            if logp_m > logp_b:\n",
    "                pred.append('M')\n",
    "            else:\n",
    "                pred.append('B')\n",
    "        return pred\n",
    "\n",
    "    def pred_accuracy(self, test_data_dict):\n",
    "        correct_pred = 0\n",
    "        N = 0\n",
    "        for label, X_test in test_data_dict.items():\n",
    "            N += X_test.shape[0]\n",
    "            pred = self.predict(X_test)\n",
    "            correct_pred += pred.count(label)\n",
    "        pred_accuracy = (correct_pred / N) * 100\n",
    "        return pred_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-calibration",
   "metadata": {},
   "source": [
    "### Fitting the model using two features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "institutional-puzzle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T06:13:38.294203Z",
     "start_time": "2021-09-03T06:13:38.201887Z"
    }
   },
   "outputs": [],
   "source": [
    "path_tr_csv = r\"Cancer_train.csv\"\n",
    "path_test_csv =  r\"Cancer_test.csv\"\n",
    "# training_data = process_csv(path_tr_csv)\n",
    "# class_M_tr_data_arr = training_data.select_class('M', num_of_features_is_2=True,scaling=False)\n",
    "# class_B_tr_data_arr = training_data.select_class('B', num_of_features_is_2=True,scaling=False)\n",
    "# model = Naive_Bayes_Classifier(class_M_tr_data_arr,\n",
    "#                                         class_B_tr_data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "protective-andrews",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T06:13:38.405843Z",
     "start_time": "2021-09-03T06:13:38.294203Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_model_get_acc(path_tr, path_test, num_of_features_is_2,scaling):\n",
    "    training_data = process_csv(path_tr)\n",
    "    class_M_tr_data_arr = training_data.select_class('M', num_of_features_is_2,scaling)\n",
    "    class_B_tr_data_arr = training_data.select_class('B', num_of_features_is_2,scaling)\n",
    "    test_data = process_csv(path_test)\n",
    "    class_M_test_data_arr = test_data.select_class('M', num_of_features_is_2,scaling)\n",
    "    class_B_test_data_arr = test_data.select_class('B', num_of_features_is_2,scaling)\n",
    "    #Fitting the model for two features.\n",
    "    classifier = Naive_Bayes_Classifier(class_M_tr_data_arr,\n",
    "                                        class_B_tr_data_arr)\n",
    "    classifier.fit()\n",
    "    print(classifier.params)\n",
    "    #FInding the accuracy of the prediction using labelled test data.\n",
    "    test_data = {'M': class_M_test_data_arr, 'B': class_B_test_data_arr}\n",
    "    pred_accuracy = classifier.pred_accuracy(test_data)\n",
    "    return (round(pred_accuracy, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tribal-pregnancy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T06:13:38.681535Z",
     "start_time": "2021-09-03T06:13:38.405843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': {'Prior': 0.3755020080321285, 'Mean': array([ 0.18146984, 17.36069519]), 'Std': array([0.04524136, 3.04235151])}, 'B': {'Prior': 0.6244979919678715, 'Mean': array([ 0.07428264, 12.15746302]), 'Std': array([0.0355992 , 1.75488937])}}\n",
      "The accuracy of the model fitted using two features(without scaling) is: 92.86\n"
     ]
    }
   ],
   "source": [
    "accuracy = fit_model_get_acc(path_tr_csv,\n",
    "                             path_test_csv,\n",
    "                             num_of_features_is_2=True,scaling=False)\n",
    "print('The accuracy of the model fitted using two features(without scaling) is:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "desperate-sheet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T06:13:38.743701Z",
     "start_time": "2021-09-03T06:13:38.685537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': {'Prior': 0.3755020080321285, 'Mean': array([0.58196191, 0.37358364]), 'Std': array([0.17267036, 0.17729321])}, 'B': {'Prior': 0.6244979919678715, 'Mean': array([0.42447223, 0.47625936]), 'Std': array([0.20342399, 0.16145822])}}\n",
      "The accuracy of the model fitted using two features(with scaling) is: 72.86\n"
     ]
    }
   ],
   "source": [
    "accuracy = fit_model_get_acc(path_tr_csv,\n",
    "                             path_test_csv,\n",
    "                             num_of_features_is_2=True,scaling = True)\n",
    "print('The accuracy of the model fitted using two features(with scaling) is:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-ivory",
   "metadata": {},
   "source": [
    "### Fitting the model using all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fundamental-polls",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T06:13:38.947701Z",
     "start_time": "2021-09-03T06:13:38.743701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': {'Prior': 0.3755020080321285, 'Mean': array([2.16757754e+01, 1.14680909e+02, 9.63925668e+02, 1.03207861e-01,\n",
      "       1.45522941e-01, 1.60242086e-01, 8.73688770e-02, 1.92603743e-01,\n",
      "       6.28813904e-02, 6.00933155e-01, 1.20367540e+00, 4.25386631e+00,\n",
      "       7.04881818e+01, 6.76953476e-03, 3.25637807e-02, 4.20966310e-02,\n",
      "       1.50284866e-02, 2.03546096e-02, 4.08903209e-03, 2.09856150e+01,\n",
      "       2.93179144e+01, 1.40272781e+02, 1.39690160e+03, 1.45265722e-01,\n",
      "       3.76061230e-01, 4.51971016e-01, 1.81469840e-01, 3.22417112e-01,\n",
      "       9.17448128e-02]), 'Std': array([3.57142728e+00, 2.07116443e+01, 3.38446779e+02, 1.26657444e-02,\n",
      "       5.41539879e-02, 7.38036696e-02, 3.29195482e-02, 2.80912919e-02,\n",
      "       7.59419383e-03, 3.25573409e-01, 4.82332341e-01, 2.43512361e+00,\n",
      "       5.34282106e+01, 2.94496609e-03, 1.90931328e-02, 2.24378078e-02,\n",
      "       5.62771642e-03, 1.03143788e-02, 2.11773069e-03, 4.00944647e+00,\n",
      "       5.15325902e+00, 2.74803176e+01, 5.41468371e+02, 2.17902914e-02,\n",
      "       1.71725863e-01, 1.84986568e-01, 4.52413608e-02, 7.40702819e-02,\n",
      "       2.17947161e-02])}, 'B': {'Prior': 0.6244979919678715, 'Mean': array([1.79454984e+01, 7.81210611e+01, 4.63481994e+02, 9.22845659e-02,\n",
      "       7.94996785e-02, 4.55827910e-02, 2.54434855e-02, 1.73917042e-01,\n",
      "       6.28320579e-02, 2.82906431e-01, 1.23490032e+00, 1.98517910e+00,\n",
      "       2.10875080e+01, 7.24272990e-03, 2.13781736e-02, 2.62131412e-02,\n",
      "       9.97184244e-03, 2.05507685e-02, 3.66177910e-03, 1.33840868e+01,\n",
      "       2.35668167e+01, 8.69733441e+01, 5.59410289e+02, 1.24735595e-01,\n",
      "       1.80682186e-01, 1.64315109e-01, 7.42826399e-02, 2.69040514e-01,\n",
      "       7.92968167e-02]), 'Std': array([3.90478991e+00, 1.15702731e+01, 1.32850190e+02, 1.35380903e-02,\n",
      "       3.36547724e-02, 4.45637589e-02, 1.55332397e-02, 2.48641026e-02,\n",
      "       6.98218115e-03, 1.11285149e-01, 5.88732353e-01, 7.43363450e-01,\n",
      "       8.67308079e+00, 3.11682650e-03, 1.68134532e-02, 3.46031799e-02,\n",
      "       5.89239687e-03, 7.04009510e-03, 3.08627932e-03, 1.95209101e+00,\n",
      "       5.32994204e+00, 1.32263663e+01, 1.62044341e+02, 2.03984277e-02,\n",
      "       9.15416514e-02, 1.42846407e-01, 3.55991987e-02, 4.24660731e-02,\n",
      "       1.42339292e-02])}}\n",
      "The accuracy of the model fitted using all features(without scaling) is: 90.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = fit_model_get_acc(path_tr_csv,\n",
    "                             path_test_csv,\n",
    "                             num_of_features_is_2=False,scaling= False)\n",
    "print('The accuracy of the model fitted using all features(without scaling) is:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "chemical-distributor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T06:13:39.096172Z",
     "start_time": "2021-09-03T06:13:38.947701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': {'Prior': 0.3755020080321285, 'Mean': array([0.35727548, 0.36690317, 0.28180297, 0.41552135, 0.32035411,\n",
      "       0.33827041, 0.37071633, 0.35683455, 0.27214386, 0.15196072,\n",
      "       0.26250831, 0.14142528, 0.11043213, 0.14413571, 0.1901257 ,\n",
      "       0.23410371, 0.27583515, 0.17550247, 0.25542688, 0.41453511,\n",
      "       0.40763328, 0.42802778, 0.30397811, 0.42451051, 0.32259308,\n",
      "       0.37345859, 0.58196191, 0.32705916, 0.24075044]), 'Std': array([0.13039165, 0.17762988, 0.15834508, 0.1784159 , 0.18414087,\n",
      "       0.18321749, 0.18198656, 0.16218991, 0.15994511, 0.12151889,\n",
      "       0.15045146, 0.11794651, 0.10443152, 0.10346647, 0.15036568,\n",
      "       0.16897212, 0.15752439, 0.14513394, 0.18018639, 0.20404308,\n",
      "       0.17109094, 0.21319098, 0.18518703, 0.16215427, 0.17058465,\n",
      "       0.16141653, 0.17267036, 0.14600883, 0.14295367])}, 'B': {'Prior': 0.6244979919678715, 'Mean': array([0.3129276 , 0.48483351, 0.37707046, 0.35799012, 0.29395501,\n",
      "       0.11096103, 0.29814255, 0.40354749, 0.25016077, 0.22272145,\n",
      "       0.19331248, 0.28162786, 0.1983273 , 0.27570075, 0.18364418,\n",
      "       0.0661948 , 0.18889643, 0.21208699, 0.09559371, 0.4587121 ,\n",
      "       0.37000404, 0.47676808, 0.36515446, 0.41385764, 0.27508866,\n",
      "       0.1312421 , 0.42447223, 0.42239111, 0.25791644]), 'Std': array([0.16911173, 0.16339886, 0.15655219, 0.12221802, 0.16455492,\n",
      "       0.10848043, 0.18201593, 0.1477368 , 0.15904741, 0.14460129,\n",
      "       0.13011235, 0.17045711, 0.12411037, 0.15539844, 0.16143808,\n",
      "       0.08738177, 0.11161957, 0.13559244, 0.10662491, 0.16417923,\n",
      "       0.18436327, 0.17246533, 0.15812289, 0.15760201, 0.16416788,\n",
      "       0.11409457, 0.20342399, 0.15952695, 0.15241385])}}\n",
      "The accuracy of the model fitted using all features(with scaling) is: 62.86\n"
     ]
    }
   ],
   "source": [
    "accuracy = fit_model_get_acc(path_tr_csv,\n",
    "                             path_test_csv,\n",
    "                             num_of_features_is_2=False,scaling=True)\n",
    "print('The accuracy of the model fitted using all features(with scaling) is:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "217px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
